{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOnIxrKXBf68LxDtYeEJsKZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGS1RWCAoDeL","executionInfo":{"status":"ok","timestamp":1725867604526,"user_tz":-540,"elapsed":9061,"user":{"displayName":"백현지","userId":"02510457579208942463"}},"outputId":"bc0f95be-d14c-42a1-b20e-978a95839062"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Collecting datasets\n","  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.21.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"]}],"source":["!pip install transformers datasets torch"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import torch\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split\n","from torch.optim import AdamW\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","import os"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpfKT8SAoKOd","executionInfo":{"status":"ok","timestamp":1725867662194,"user_tz":-540,"elapsed":45383,"user":{"displayName":"백현지","userId":"02510457579208942463"}},"outputId":"7d5121ea-7bdb-488e-87d4-f229e3e5824d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import ElectraTokenizer, ElectraForSequenceClassification, AdamW\n","from sklearn.model_selection import train_test_split\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","import numpy as np\n","\n","# 데이터 파일 경로\n","file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/final_dataset_test.csv'\n","\n","# 데이터 읽어오기\n","data = pd.read_csv(file_path)\n","\n","X = data.drop(columns=['MSCI_Score'])\n","\n","# y 값은 MSCI_Score\n","y = data['MSCI_Score']\n","\n","# KoELECTRA 모델의 토크나이저 로드\n","tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n","\n","# 데이터셋 클래스 정의\n","class FeatureDataset(Dataset):\n","    def __init__(self, features, labels, tokenizer, max_len):\n","        # Features 처리\n","        if isinstance(features, pd.DataFrame):\n","            self.features = features.reset_index(drop=True)\n","        else:\n","            self.features = features  # 리스트나 다른 형식일 경우 그대로 사용\n","\n","        # Labels 처리\n","        if isinstance(labels, (pd.Series, pd.DataFrame)):\n","            self.labels = labels.reset_index(drop=True)\n","        else:\n","            self.labels = labels  # 리스트나 다른 형식일 경우 그대로 사용\n","\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        # Features에서 텍스트와 수치형 피처 추출\n","        if isinstance(self.features, pd.DataFrame):\n","            text = self.features.iloc[idx]['full_text']\n","            numeric_features = self.features.iloc[idx].drop(labels=['full_text', 'Company', 'date', 'Year']).astype(float).values\n","        else:\n","            # features가 리스트인 경우 (예: 리스트의 딕셔너리)\n","            text = self.features[idx]['full_text']\n","            # 'full_text', 'Company', 'date', 'Year'를 제외한 수치형 피처 추출\n","            numeric_features = [float(value) for key, value in self.features[idx].items() if key not in ['full_text', 'Company', 'date', 'Year']]\n","            numeric_features = np.array(numeric_features)\n","\n","        # Labels에서 라벨 추출\n","        if isinstance(self.labels, (pd.Series, pd.DataFrame)):\n","            label = self.labels.iloc[idx]\n","        else:\n","            label = self.labels[idx]\n","\n","        # 텍스트 토큰화 (clean_up_tokenization_spaces 제거)\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'numeric_features': torch.tensor(numeric_features, dtype=torch.float),\n","            'labels': torch.tensor(label, dtype=torch.float)\n","        }\n","\n","# 학습셋과 검증셋으로 분리\n","train_features, val_features, train_labels, val_labels = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 하이퍼파라미터 탐색을 위한 값들 설정\n","max_len_values = [128, 256]\n","batch_size_values = [16, 32]\n","learning_rate_values = [2e-5, 3e-5, 5e-5]\n","num_epochs = 50\n","\n","# 디바이스 설정\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# 텐서를 연속적으로 변환하는 함수\n","def make_contiguous(model):\n","    for param in model.parameters():\n","        param.data = param.data.contiguous()\n","\n","# 훈련 함수 정의\n","def train(model, train_dataloader, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(train_dataloader, desc=\"Training\"):\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        numeric_features = batch['numeric_features'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # KoELECTRA 모델의 텍스트 출력\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits.squeeze()\n","\n","        # 텍스트 출력과 수치형 피처 결합\n","        combined_output = torch.cat((logits.unsqueeze(1), numeric_features), dim=1)\n","\n","        # 손실 계산 (MSE Loss)\n","        loss = F.mse_loss(combined_output.sum(dim=1), labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_dataloader)\n","\n","# 평가 함수 정의\n","def evaluate(model, val_dataloader, device):\n","    model.eval()\n","    total_loss = 0\n","    all_labels = []\n","    all_preds = []\n","    with torch.no_grad():\n","        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            numeric_features = batch['numeric_features'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits.squeeze()\n","\n","            combined_output = torch.cat((logits.unsqueeze(1), numeric_features), dim=1)\n","            loss = F.mse_loss(combined_output.sum(dim=1), labels)\n","\n","            total_loss += loss.item()\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(combined_output.sum(dim=1).cpu().numpy())\n","\n","    return total_loss / len(val_dataloader), all_labels, all_preds\n","\n","# 하이퍼파라미터 탐색 루프\n","for max_len in max_len_values:\n","    for batch_size in batch_size_values:\n","        for learning_rate in learning_rate_values:\n","            print(f\"Training with max_len={max_len}, batch_size={batch_size}, learning_rate={learning_rate}\")\n","\n","            # 데이터셋 객체 생성\n","            train_dataset = FeatureDataset(train_features, train_labels, tokenizer, max_len)\n","            val_dataset = FeatureDataset(val_features, val_labels, tokenizer, max_len)\n","\n","            # DataLoader 생성\n","            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","            val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","            # KoELECTRA 모델 로드\n","            model = ElectraForSequenceClassification.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=1)\n","            model.to(device)\n","            optimizer = AdamW(model.parameters(), lr=learning_rate)\n","\n","            # 조기 종료를 위한 변수\n","            best_val_loss = float('inf')\n","            patience = 5\n","            patience_counter = 0\n","\n","            # 훈련 및 평가\n","            for epoch in range(num_epochs):\n","                print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","                train_loss = train(model, train_dataloader, optimizer, device)\n","                val_loss, val_labels_out, val_preds = evaluate(model, val_dataloader, device)\n","                print(f\"Training Loss: {train_loss:.4f}\")\n","                print(f\"Validation Loss: {val_loss:.4f}\")\n","\n","                if val_loss < best_val_loss:\n","                    print(f\"Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model...\")\n","                    best_val_loss = val_loss\n","                    patience_counter = 0\n","\n","                    # 최선의 모델 저장\n","                    best_model_dir = f\"/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len{max_len}_batch{batch_size}_lr{learning_rate}\"\n","                    if not os.path.exists(best_model_dir):\n","                        os.makedirs(best_model_dir)\n","                    make_contiguous(model)\n","                    model.save_pretrained(best_model_dir)\n","                    tokenizer.save_pretrained(best_model_dir)\n","                    print(f\"Best model and tokenizer saved to {best_model_dir}\")\n","                else:\n","                    patience_counter += 1\n","                    print(f\"Validation loss did not improve. Patience counter: {patience_counter}/{patience}\")\n","\n","                if patience_counter >= patience:\n","                    print(\"Early stopping triggered. Stopping training...\")\n","                    break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4i5mvxypEx8","outputId":"82a794be-600b-4fcd-ff72-8a5daab3a113"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training with max_len=128, batch_size=16, learning_rate=2e-05\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 303/303 [00:57<00:00,  5.25it/s]\n","Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 5.8328\n","Validation Loss: 2.8752\n","Validation loss improved from inf to 2.8752. Saving model...\n","Best model and tokenizer saved to /content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len128_batch16_lr2e-05\n","Epoch 2/50\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 303/303 [00:58<00:00,  5.14it/s]\n","Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 2.4812\n","Validation Loss: 2.3825\n","Validation loss improved from 2.8752 to 2.3825. Saving model...\n","Best model and tokenizer saved to /content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len128_batch16_lr2e-05\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 303/303 [00:58<00:00,  5.15it/s]\n","Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.9903\n","Validation Loss: 2.3327\n","Validation loss improved from 2.3825 to 2.3327. Saving model...\n","Best model and tokenizer saved to /content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len128_batch16_lr2e-05\n","Epoch 4/50\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 303/303 [00:58<00:00,  5.16it/s]\n","Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.6659\n","Validation Loss: 2.5637\n","Validation loss did not improve. Patience counter: 1/5\n","Epoch 5/50\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 303/303 [00:58<00:00,  5.15it/s]\n","Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.3392\n","Validation Loss: 2.4069\n","Validation loss did not improve. Patience counter: 2/5\n","Epoch 6/50\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 303/303 [00:58<00:00,  5.15it/s]\n","Evaluating: 100%|██████████| 76/76 [00:10<00:00,  7.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.1364\n","Validation Loss: 2.3098\n","Validation loss improved from 2.3327 to 2.3098. Saving model...\n","Best model and tokenizer saved to /content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/best_model_len128_batch16_lr2e-05\n","Epoch 7/50\n"]},{"output_type":"stream","name":"stderr","text":["Training:  86%|████████▋ | 262/303 [00:51<00:07,  5.27it/s]"]}]},{"cell_type":"code","source":["# 저장된 모델과 토크나이저 경로\n","output_dir = \"/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/\"\n","\n","# 모델 로드\n","model = ElectraForSequenceClassification.from_pretrained(output_dir)\n","model.eval()  # 평가 모드로 전환\n","\n","# 토크나이저 로드\n","tokenizer = ElectraTokenizer.from_pretrained(output_dir)\n","\n","# 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# 새로운 CSV 파일 읽기\n","new_file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/sample_dataset_test.csv'\n","new_data = pd.read_csv(new_file_path)\n","\n","# 데이터셋 클래스 정의 (훈련에서 사용한 클래스와 동일해야 함)\n","class FeatureDataset(Dataset):\n","    def __init__(self, features, tokenizer, max_len):\n","        self.features = features\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, idx):\n","        # 텍스트와 그 외의 수치형 피처들을 분리\n","        text = self.features.iloc[idx]['full_text']\n","\n","        # 수치형 피처들만 선택 (숫자 데이터만 선택)\n","        numeric_features = self.features.iloc[idx].drop(labels=['full_text', 'Company', 'date', 'Year']).astype(float).values\n","\n","        # 텍스트 토큰화\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        # 반환\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'numeric_features': torch.tensor(numeric_features, dtype=torch.float),\n","        }\n","\n","# 파라미터 설정\n","max_len = 128\n","batch_size = 16\n","\n","# 데이터셋 객체 생성\n","predict_dataset = FeatureDataset(new_data, tokenizer, max_len)\n","\n","# DataLoader 생성\n","predict_dataloader = DataLoader(predict_dataset, batch_size=batch_size, shuffle=False)\n","\n","# 예측 함수 정의\n","def predict(model, dataloader, device):\n","    model.eval()\n","    predictions = []\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Predicting\"):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            numeric_features = batch['numeric_features'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits.squeeze()\n","\n","            combined_output = torch.cat((logits.unsqueeze(1), numeric_features), dim=1)\n","            final_output = combined_output.sum(dim=1)  # 예: 간단히 합산하여 최종 출력 계산\n","\n","            predictions.extend(final_output.cpu().numpy())\n","\n","    return predictions\n","\n","# 예측 수행\n","predictions = predict(model, predict_dataloader, device)\n","\n","# 예측 값 분포 확인\n","predicted_label_distribution = pd.Series(predictions).value_counts(bins=10)\n","print(\"Predicted label distribution:\")\n","print(predicted_label_distribution)\n"],"metadata":{"id":"oHfFgarr3Ngj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723794660842,"user_tz":-540,"elapsed":96076,"user":{"displayName":"백현지","userId":"02510457579208942463"}},"outputId":"5fd1cfcd-d4a2-41b1-db9e-52eb1e65ce3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Predicting: 100%|██████████| 441/441 [01:25<00:00,  5.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Predicted label distribution:\n","(3.645, 4.828]      1533\n","(1.279, 2.462]      1502\n","(4.828, 6.011]      1348\n","(2.462, 3.645]       955\n","(0.0965, 1.279]      873\n","(6.011, 7.194]       509\n","(-1.086, 0.0965]     155\n","(7.194, 8.377]       148\n","(8.377, 9.56]         15\n","(-2.282, -1.086]      12\n","Name: count, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# 예측 결과를 Pandas Series로 변환\n","predictions_series = pd.Series(predictions)\n","\n","# 결측치 확인\n","missing_values = predictions_series.isnull().sum()\n","print(f\"Number of missing values in predictions: {missing_values}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RI2U6NIgKy1M","executionInfo":{"status":"ok","timestamp":1723794671130,"user_tz":-540,"elapsed":439,"user":{"displayName":"백현지","userId":"02510457579208942463"}},"outputId":"cf22bbaf-7b07-4ce6-abcc-daf18c4293d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of missing values in predictions: 0\n"]}]},{"cell_type":"code","source":["# 예측 결과를 데이터프레임에 추가\n","new_data['predicted_label'] = predictions\n","\n","# 예측 결과가 포함된 데이터 저장\n","output_file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/결과/predictions.csv'\n","new_data.to_csv(output_file_path, index=False, encoding='utf-8-sig')\n","\n","print(f\"Predictions saved to {output_file_path}\")\n"],"metadata":{"id":"J1TH1C613VOV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723794680413,"user_tz":-540,"elapsed":2185,"user":{"displayName":"백현지","userId":"02510457579208942463"}},"outputId":"0df68bc8-c292-4547-d638-42aee8aff676"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions saved to /content/drive/MyDrive/Kwargs/모델B1/결과/predictions.csv\n"]}]},{"cell_type":"code","source":["# CSV 파일 경로\n","file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/결과/predictions.csv'\n","\n","# 데이터 읽어오기\n","data = pd.read_csv(file_path)\n","\n","# 필요한 열만 선택\n","data = data[['Year', 'Company', 'predicted_label']]\n","\n","# Company와 Year로 그룹화한 뒤 predicted_label의 평균 계산\n","grouped_data = data.groupby(['Company', 'Year']).agg(average_label=('predicted_label', 'mean')).reset_index()\n","print(grouped_data.to_string())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2V7yPsCHj_t","executionInfo":{"status":"ok","timestamp":1723794689217,"user_tz":-540,"elapsed":1103,"user":{"displayName":"백현지","userId":"02510457579208942463"}},"outputId":"5da71ca8-883b-437e-e507-2117bc7a4e1a","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["     Company  Year  average_label\n","0       KB금융  2019       5.016282\n","1       KB금융  2020       5.109671\n","2       KB금융  2021       5.996064\n","3       KB금융  2022       6.966534\n","4       KB금융  2023       7.012989\n","5      NAVER  2019       4.815658\n","6      NAVER  2020       4.797409\n","7      NAVER  2021       6.733145\n","8      NAVER  2022       6.642238\n","9      NAVER  2023       5.643522\n","10    SK하이닉스  2019       2.097431\n","11    SK하이닉스  2020       3.649572\n","12    SK하이닉스  2021       3.710508\n","13    SK하이닉스  2022       4.903346\n","14    SK하이닉스  2023       4.920477\n","15        기아  2019       1.796560\n","16        기아  2020       1.301239\n","17        기아  2021       2.058400\n","18        기아  2022       1.960288\n","19        기아  2023       2.774900\n","20     삼성SDI  2019       4.350137\n","21     삼성SDI  2020       4.419508\n","22     삼성SDI  2021       4.213435\n","23     삼성SDI  2022       4.739213\n","24     삼성SDI  2023       4.776360\n","25      삼성물산  2019       3.249006\n","26      삼성물산  2020       4.275316\n","27      삼성물산  2021       4.703798\n","28      삼성물산  2022       4.477484\n","29      삼성물산  2023       4.801013\n","30  삼성바이오로직스  2019       2.390249\n","31  삼성바이오로직스  2020       3.275905\n","32  삼성바이오로직스  2021       2.203565\n","33  삼성바이오로직스  2022       2.277998\n","34  삼성바이오로직스  2023       1.842113\n","35      삼성생명  2019       1.869106\n","36      삼성생명  2020       3.027084\n","37      삼성생명  2021       4.745852\n","38      삼성생명  2022       4.388641\n","39      삼성생명  2023       2.771990\n","40      삼성전자  2019       4.225913\n","41      삼성전자  2020       5.421561\n","42      삼성전자  2021       5.118524\n","43      삼성전자  2022       4.948362\n","44      삼성전자  2023       5.915540\n","45      셀트리온  2019       2.846863\n","46      셀트리온  2020       2.165246\n","47      셀트리온  2021       1.668776\n","48      셀트리온  2022       0.632931\n","49      셀트리온  2023       0.620857\n","50      신한지주  2019       5.687663\n","51      신한지주  2020       5.470094\n","52      신한지주  2021       5.464048\n","53      신한지주  2022       5.390045\n","54      신한지주  2023       5.971607\n","55       카카오  2019       3.345765\n","56       카카오  2020       3.933448\n","57       카카오  2021       4.670168\n","58       카카오  2022       5.936382\n","59       카카오  2023       4.776292\n","60   포스코 홀딩스  2019       3.363093\n","61   포스코 홀딩스  2020       3.434434\n","62   포스코 홀딩스  2021       3.483677\n","63   포스코 홀딩스  2022       3.435544\n","64   포스코 홀딩스  2023       4.437256\n","65     현대모비스  2019       0.538835\n","66     현대모비스  2020       1.869277\n","67     현대모비스  2021       1.215652\n","68     현대모비스  2022       1.194249\n","69     현대모비스  2023       1.688000\n","70       현대차  2019       1.757339\n","71       현대차  2020       1.671593\n","72       현대차  2021       1.222235\n","73       현대차  2022       1.381263\n","74       현대차  2023       2.590015\n"]}]},{"cell_type":"code","source":["# 결과를 새로운 CSV 파일로 저장\n","output_file_path = '/content/drive/MyDrive/Kwargs/050. 기관 점수 예측 모델 (모델 B1)/결과/average_predictions.csv'\n","grouped_data.to_csv(output_file_path, index=False)\n","\n","print(f\"결과가 {output_file_path}에 저장되었습니다.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvA6so_CIFPN","executionInfo":{"status":"ok","timestamp":1723794696332,"user_tz":-540,"elapsed":1070,"user":{"displayName":"백현지","userId":"02510457579208942463"}},"outputId":"a5f4bcb6-84dd-44d0-9277-d1f4b00c3873"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["결과가 /content/drive/MyDrive/Kwargs/모델B1/결과/average_predictions.csv에 저장되었습니다.\n"]}]}]}