{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bLmpQnFg36V9"},"outputs":[],"source":["# 필요한 라이브러리 설치\n","!pip install transformers torch\n","!pip install --upgrade gluonnlp pandas tqdm\n","!pip install mxnet\n","!pip install transformers[torch]\n","!pip install accelerate -U\n","\n","# 라이브러리 임포트\n","import pandas as pd\n","import csv\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","from transformers import AutoModel, AutoTokenizer, Trainer, TrainingArguments\n","from transformers import ElectraModel, ElectraTokenizer\n","import numpy as np\n","from tqdm.auto import tqdm\n","from google.colab import drive\n","import glob\n","import os\n","from tqdm.auto import tqdm\n","from multiprocessing import Pool\n","import multiprocessing\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Google Drive 마운트\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkeICgA84Ho-","executionInfo":{"status":"ok","timestamp":1725004388322,"user_tz":-540,"elapsed":1535339,"user":{"displayName":"백현지","userId":"02510457579208942463"}},"outputId":"79fea87f-cfcc-4797-9c9b-a47c45964823"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 2355/2355 [06:33<00:00,  5.98it/s]\n","Evaluating: 100%|██████████| 590/590 [00:31<00:00, 18.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.4277\n","Validation Loss: 1.3433\n","Validation Accuracy: 0.4792\n","Model and tokenizer saved to /content/drive/MyDrive/Kwargs/esg관련도/모델_epoch_1 at Epoch 1\n","Epoch 2/3\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 2355/2355 [06:33<00:00,  5.98it/s]\n","Evaluating: 100%|██████████| 590/590 [00:31<00:00, 18.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.6462\n","Validation Loss: 1.6441\n","Validation Accuracy: 0.4245\n","Model and tokenizer saved to /content/drive/MyDrive/Kwargs/esg관련도/모델_epoch_2 at Epoch 2\n","Epoch 3/3\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 2355/2355 [06:33<00:00,  5.98it/s]\n","Evaluating: 100%|██████████| 590/590 [00:31<00:00, 18.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Training Loss: 1.6746\n","Validation Loss: 1.6454\n","Validation Accuracy: 0.4245\n","Model and tokenizer saved to /content/drive/MyDrive/Kwargs/esg관련도/모델_epoch_3 at Epoch 3\n"]}],"source":["import os\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n","from transformers import ElectraTokenizer, ElectraModel\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","import glob\n","\n","# KoElectra 토크나이저와 모델 불러오기\n","tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-discriminator\")\n","\n","# 분류를 위한 모델 정의\n","class ElectraForCompanyClassification(nn.Module):\n","    def __init__(self, model_name, num_labels):\n","        super(ElectraForCompanyClassification, self).__init__()\n","        self.electra = ElectraModel.from_pretrained(model_name)\n","        self.dropout = nn.Dropout(0.1)\n","        self.classifier = nn.Linear(self.electra.config.hidden_size, num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.electra(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# 데이터 전처리 함수\n","def preprocess(data, tokenizer, max_len=256):\n","    # content 열을 문자열 리스트로 변환\n","    texts = data[\"full_text\"].astype(str).tolist()\n","    # esg_score 열을 0부터 10 사이의 정수로 변환\n","    labels = (data[\"esg_score\"] * 10).astype(int).tolist()\n","\n","    inputs = tokenizer(\n","        texts,\n","        max_length=max_len,\n","        padding=True,\n","        truncation=True,\n","        return_tensors=\"pt\"\n","    )\n","\n","    labels = torch.tensor(labels, dtype=torch.long)  # 분류를 위해 long 타입으로 변환\n","\n","    return inputs, labels\n","\n","# 데이터 파일 경로 설정\n","file_paths = glob.glob('/content/drive/MyDrive/Kwargs/esg관련도/*.csv')\n","\n","# 모델 초기화\n","model_name = \"monologg/koelectra-base-discriminator\"\n","num_labels = 11  # 0부터 10까지의 11개의 클래스\n","model = ElectraForCompanyClassification(model_name, num_labels)\n","\n","# 모든 파일에 대해 처리\n","all_train_datasets = []\n","all_val_datasets = []\n","\n","for file_path in file_paths:\n","    data = pd.read_csv(file_path)\n","\n","    # 데이터 전처리\n","    inputs, labels = preprocess(data, tokenizer)\n","\n","    # TensorDataset 생성\n","    dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n","\n","    # 데이터셋 분할\n","    train_size = int(0.8 * len(dataset))\n","    val_size = len(dataset) - train_size\n","\n","    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n","\n","    all_train_datasets.append(train_dataset)\n","    all_val_datasets.append(val_dataset)\n","\n","# 모든 데이터를 합침\n","train_dataset = torch.utils.data.ConcatDataset(all_train_datasets)\n","val_dataset = torch.utils.data.ConcatDataset(all_val_datasets)\n","\n","# 데이터 로더 생성\n","train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n","val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n","\n","# 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","# 훈련 함수\n","def train(model, train_dataloader, optimizer, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(train_dataloader, desc=\"Training\"):\n","        input_ids, attention_mask, labels = [x.to(device) for x in batch]\n","        optimizer.zero_grad()\n","        logits = model(input_ids, attention_mask)\n","        loss = nn.CrossEntropyLoss()(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_dataloader)\n","\n","# 평가 함수\n","def evaluate(model, val_dataloader, device):\n","    model.eval()\n","    total_loss = 0\n","    all_labels = []\n","    all_preds = []\n","    with torch.no_grad():\n","        for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n","            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n","            logits = model(input_ids, attention_mask)\n","            loss = nn.CrossEntropyLoss()(logits, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(logits, dim=1).cpu().numpy()\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds)\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    return total_loss / len(val_dataloader), accuracy\n","\n","# 훈련 및 평가\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","    train_loss = train(model, train_dataloader, optimizer, device)\n","    val_loss, val_accuracy = evaluate(model, val_dataloader, device)\n","    print(f\"Training Loss: {train_loss:.4f}\")\n","    print(f\"Validation Loss: {val_loss:.4f}\")\n","    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","\n","    # 각 에포크마다 모델 저장\n","    epoch_output_dir = f\"/content/drive/MyDrive/Kwargs/esg관련도/모델_epoch_{epoch + 1}\"\n","\n","    # 디렉토리가 존재하지 않으면 생성\n","    if not os.path.exists(epoch_output_dir):\n","        os.makedirs(epoch_output_dir)\n","\n","    # 모델 가중치 저장\n","    torch.save(model.state_dict(), os.path.join(epoch_output_dir, \"pytorch_model.bin\"))\n","    tokenizer.save_pretrained(epoch_output_dir)\n","\n","    print(f\"Model and tokenizer saved to {epoch_output_dir} at Epoch {epoch + 1}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"6YYMl1aRHV0X"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"mount_file_id":"1wwZt9Tzs8nbP_lPoyd951z5ndrY9jWh6","authorship_tag":"ABX9TyPdxKyxtW+AQHEk8OunKFN0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}